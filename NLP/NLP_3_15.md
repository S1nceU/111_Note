## N-gram
#### Perplexity
在填空句子時，預測字出現的可能性，預測的準確率好或壞
可用來選擇評估一個語言模型準確預測

假設有幾個字，就要開幾次根號
![[Pasted image 20230315104025.png]]
也可用
![[Pasted image 20230315104105.png]]

簡單來說，基數有多少，Perplexity 就是多少
越低越好

用單一數字評估語言模型的好壞

## Generalization and zeros
zeros : 沒有在 training data 裡面出現過的

## Smoothing : Add-one (Laplace) smoothing 
將每個字的出現機率都偷掉一點
使得每個字出現的機率差距縮小

![[Pasted image 20230315110159.png]]

## Interpolation, Backoff, and Web-Scale LMs
#### Backoff 
+ trigram
+ bigram
+ unigram
一步一步減少 context

#### Interpolation
+ mix unigram, bigram, trigram
將所有方式組合在一起

#### How to set the lambdas?
選用 held-out 語料庫
用 held-out data 修正 n-gram 模型

#### 出現訓練資料沒出現過的資料
##### Out of Vocabulary = OOV words
創建一個 token (**UNK**)
訓練這個出現的機率

#### Smoothing for Web-Scale N-gram
+ "Stupid backoff"

# Chapter 4
##  

